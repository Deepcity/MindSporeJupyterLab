{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb430c0e-fd70-46e2-91e2-b14cf782bd06",
   "metadata": {},
   "source": [
    "# 基于MindSpore的GPT2文本摘要\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.2.14，如需更换mindspore版本，可更改下面mindspore的版本号\n",
    "!pip uninstall mindspore -y\n",
    "!pip install -i https://pypi.mirrors.ustc.edu.cn/simple mindspore==2.2.14"
   ],
   "id": "c36235da5dc3361e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "!pip install tokenizers==0.15.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# 该案例在 mindnlp 0.3.1 版本完成适配，如果发现案例跑不通，可以指定mindnlp版本，执行`!pip install mindnlp==0.3.1`\n",
    "!pip install mindnlp"
   ],
   "id": "6491560b-1ec6-4ca1-88cc-c7f9c1297725"
  },
  {
   "cell_type": "markdown",
   "id": "bb699e4a-a2dc-44f2-b3cb-6b86fa9b24f6",
   "metadata": {},
   "source": [
    "### 数据集加载与处理\n",
    "\n",
    "1. 数据集加载\n",
    "\n",
    "    本次实验使用的是nlpcc2017摘要数据，内容为新闻正文及其摘要，总计50000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a771c04-0ba5-40be-a1fb-722b636c166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.utils import http_get\n",
    "\n",
    "# download dataset\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path = http_get(url, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5868b6-7a52-4f97-b934-4d3632a978a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "# load dataset\n",
    "dataset = TextFileDataset(str(path), shuffle=False)\n",
    "dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf79231-864c-4e11-9409-995c95cdb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing dataset\n",
    "train_dataset, test_dataset = dataset.split([0.9, 0.1], randomize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0d574-53a0-4bac-9303-6eb769418c04",
   "metadata": {},
   "source": [
    "2. 数据预处理\n",
    "\n",
    "    原始数据格式：\n",
    "    ```text\n",
    "    article: [CLS] article_context [SEP]\n",
    "    summary: [CLS] summary_context [SEP]\n",
    "    ```\n",
    "    预处理后的数据格式：\n",
    "\n",
    "    ```text\n",
    "    [CLS] article_context [SEP] summary_context [SEP]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ee1961-0658-4e70-95c2-81fefd83a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# preprocess dataset\n",
    "def process_dataset(dataset, tokenizer, batch_size=6, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def merge_and_pad(article, summary):\n",
    "        # tokenization\n",
    "        # pad to max_seq_length, only truncate the article\n",
    "        tokenized = tokenizer(text=article, text_pair=summary,\n",
    "                              padding='max_length', truncation='only_first', max_length=max_seq_len)\n",
    "        return tokenized['input_ids'], tokenized['input_ids']\n",
    "    \n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    # change column names to input_ids and labels for the following training\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], ['input_ids', 'labels'])\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce3dab-9486-4365-be7c-34bd5a761080",
   "metadata": {},
   "source": [
    "因GPT2无中文的tokenizer，我们使用BertTokenizer替代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3cd8e57-72bc-4d2e-b38d-38b24efadd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp.transformers import BertTokenizer\n",
    "\n",
    "# We use BertTokenizer for tokenizing chinese context.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e89c26b-4970-449a-a0c4-b6c61845e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b65cc13-0a52-4bae-ab5f-ebb813a4d3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[4, 1024], dtype=Int64, value=\n",
       " [[101, 100, 100 ...   0,   0,   0],\n",
       "  [101, 100, 100 ...   0,   0,   0],\n",
       "  [101, 100, 100 ... 100, 100, 102],\n",
       "  [101, 100, 100 ...   0,   0,   0]]),\n",
       " Tensor(shape=[4, 1024], dtype=Int64, value=\n",
       " [[101, 100, 100 ...   0,   0,   0],\n",
       "  [101, 100, 100 ...   0,   0,   0],\n",
       "  [101, 100, 100 ... 100, 100, 102],\n",
       "  [101, 100, 100 ...   0,   0,   0]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1497ee-2ad1-4da8-b659-9c7f2d45fccc",
   "metadata": {},
   "source": [
    "### 模型构建\n",
    "\n",
    "1. 构建GPT2ForSummarization模型，注意***shift right***的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f295944-ea2e-41e1-8301-472e09223792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops\n",
    "from mindnlp.transformers import GPT2LMHeadModel\n",
    "\n",
    "class GPT2ForSummarization(GPT2LMHeadModel):\n",
    "    def construct(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        outputs = super().construct(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        shift_logits = outputs.logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "        # Flatten the tokens\n",
    "        loss = ops.cross_entropy(shift_logits.view(-1, shift_logits.shape[-1]), shift_labels.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6af843-64d7-49a3-875f-605d6b2e74b2",
   "metadata": {},
   "source": [
    "2. 动态学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c7be3d-44dc-49d4-abd8-c41f316a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops\n",
    "from mindspore.nn.learning_rate_schedule import LearningRateSchedule\n",
    "\n",
    "class LinearWithWarmUp(LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Warmup-decay learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, num_warmup_steps, num_training_steps):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "    def construct(self, global_step):\n",
    "        if global_step < self.num_warmup_steps:\n",
    "            return global_step / float(max(1, self.num_warmup_steps)) * self.learning_rate\n",
    "        return ops.maximum(\n",
    "            0.0, (self.num_training_steps - global_step) / (max(1, self.num_training_steps - self.num_warmup_steps))\n",
    "        ) * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e9db2-11df-4cc4-8bef-87d473d99e5a",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a655320-2d05-4c93-bc8b-f1b4f45f809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "warmup_steps = 2000\n",
    "learning_rate = 1.5e-4\n",
    "\n",
    "num_training_steps = num_epochs * train_dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ac9003-2dcf-42f8-b42d-3a788f172d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "from mindnlp.transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2ForSummarization(config)\n",
    "\n",
    "lr_scheduler = LinearWithWarmUp(learning_rate=learning_rate, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2803c71c-3591-48cf-a6a9-6b840af749bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model parameters: 94500096\n"
     ]
    }
   ],
   "source": [
    "# 记录模型参数数量\n",
    "print('number of model parameters: {}'.format(model.num_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a1824-7696-40f8-9a2d-9638d3a1fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp._legacy.engine import Trainer\n",
    "from mindnlp._legacy.engine.callbacks import CheckpointCallback\n",
    "\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='gpt2_summarization',\n",
    "                                epochs=1, keep_checkpoint_max=2)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=train_dataset,\n",
    "                  epochs=1, optimizer=optimizer, callbacks=ckpoint_cb)\n",
    "trainer.set_amp(level='O1')  # 开启混合精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054b2fb-2ddc-4a41-b93d-b57ddec7e069",
   "metadata": {},
   "source": [
    "***注：建议使用较高规格的算力，训练时间较长***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c124d-2fc8-409e-b2d4-e10bf9cc2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(tgt_columns=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5c3fc-f4c1-4624-9489-1593faad4384",
   "metadata": {},
   "source": [
    "## 模型推理\n",
    "数据处理，将向量数据变为中文数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787795ec-0c07-4be6-97b7-4defbe899117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_dataset(dataset, tokenizer, batch_size=1, max_seq_len=1024, max_summary_len=100):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def pad(article):\n",
    "        tokenized = tokenizer(text=article, truncation=True, max_length=max_seq_len-max_summary_len)\n",
    "        return tokenized['input_ids']\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(pad, 'article', ['input_ids'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996842c4-f793-4393-ae64-2d4b065bc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = process_test_dataset(test_dataset, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d8eea-9fe4-4cd8-abb2-acf18dd62b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(test_dataset.create_tuple_iterator(output_numpy=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a5288a4-f584-479b-9653-bda060e00279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('./checkpoint/gpt2_summarization_epoch_0.ckpt', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf93b2e-f2f2-407b-aa34-76f6aa80d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_train(False)\n",
    "model.config.eos_token_id = model.config.sep_token_id\n",
    "i = 0\n",
    "for (input_ids, raw_summary) in test_dataset.create_tuple_iterator():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=50, num_beams=5, no_repeat_ngram_size=2)\n",
    "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "    print(output_text)\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e8a59-a13f-4caf-8418-c090d0438729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "f12fdc30-1f88-4d5d-ba7e-453355155c16"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
